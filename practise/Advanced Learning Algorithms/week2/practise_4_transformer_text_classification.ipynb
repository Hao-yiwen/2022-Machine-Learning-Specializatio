{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载和预处理数据...\n",
      "\n",
      "创建Transformer模型...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│                     │                   │            │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m768\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │    \u001b[38;5;34m132,672\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│                     │                   │            │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m128\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m8,320\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m8,256\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m128\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,497</span> (603.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m154,497\u001b[0m (603.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,497</span> (603.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m154,497\u001b[0m (603.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练...\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5443 - loss: 0.7021 - val_accuracy: 0.6250 - val_loss: 0.6982 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5964 - loss: 0.6735 - val_accuracy: 0.3750 - val_loss: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4913 - loss: 0.7653 - val_accuracy: 0.6250 - val_loss: 0.6652 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4861 - loss: 0.7232 - val_accuracy: 0.6250 - val_loss: 0.6576 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5955 - loss: 0.6782 - val_accuracy: 0.3750 - val_loss: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5621 - loss: 0.6688 - val_accuracy: 0.6250 - val_loss: 0.6641 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5699 - loss: 0.7084 - val_accuracy: 0.6250 - val_loss: 0.6492 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5621 - loss: 0.6981 - val_accuracy: 0.6250 - val_loss: 0.6484 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5629 - loss: 0.7252 - val_accuracy: 0.6250 - val_loss: 0.6482 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5304 - loss: 0.6849 - val_accuracy: 0.6250 - val_loss: 0.6566 - learning_rate: 5.0000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7500 - loss: 0.6007\n",
      "\n",
      "测试集准确率: 0.7500\n",
      "\n",
      "预测新评论:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "文本: '这个产品非常好用，超出我的预期'\n",
      "预测: 正面评价 (置信度: 0.5982)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "文本: '质量很差，客服态度也不好'\n",
      "预测: 正面评价 (置信度: 0.5982)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_transformer_model(vocab_size, max_length):\n",
    "    # 输入层\n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # 嵌入层\n",
    "    embedding_dim = 64\n",
    "    x = layers.Embedding(vocab_size, embedding_dim, input_length=max_length)(inputs)\n",
    "    \n",
    "    # 添加位置信息\n",
    "    x = layers.Embedding(input_dim=max_length, output_dim=embedding_dim)(\n",
    "        tf.range(start=0, limit=max_length, delta=1)\n",
    "    ) + x\n",
    "    \n",
    "    # 多头注意力层\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=8, \n",
    "        key_dim=embedding_dim\n",
    "    )(x, x)\n",
    "    \n",
    "    # 添加残差连接和层归一化\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(attention_output + x)\n",
    "    \n",
    "    # 前馈网络\n",
    "    ffn = layers.Dense(128, activation='relu')(x)\n",
    "    ffn = layers.Dense(embedding_dim)(ffn)\n",
    "    \n",
    "    # 再次添加残差连接和层归一化\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(ffn + x)\n",
    "    \n",
    "    # 全局池化\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # 全连接层\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # 示例数据\n",
    "    data = {\n",
    "        'review': [\n",
    "            \"这个产品很好用，我很喜欢\",\n",
    "            \"质量特别差，退货了\",\n",
    "            \"一般般，可以接受\",\n",
    "            \"很满意，物超所值\",\n",
    "            \"不推荐购买，浪费钱\",\n",
    "            \"服务态度很好，下次还会购买\",\n",
    "            \"出现故障，客服态度很差\",\n",
    "            \"性价比很高，推荐购买\",\n",
    "            \"完全不值这个价格\",\n",
    "            \"快递很快，产品完好\",\n",
    "        ] * 10,  # 复制数据以增加数据量\n",
    "        'label': [1, 0, 1, 1, 0, 1, 0, 1, 0, 1] * 10\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # 分割数据\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df['review'].values, \n",
    "        df['label'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 文本处理\n",
    "    tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # 转换为序列\n",
    "    max_length = 50\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "    \n",
    "    # 填充\n",
    "    X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "    X_test = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    return (X_train, train_labels), (X_test, test_labels), tokenizer, max_length\n",
    "\n",
    "def train_model(model, train_data, test_data, batch_size=32, epochs=10):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # 创建验证集\n",
    "    X_train_main, X_val, y_train_main, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 回调函数\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(\n",
    "        X_train_main, y_train_main,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # 评估模型\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\n测试集准确率: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_sentiment(model, text, tokenizer, max_length):\n",
    "    # 预处理文本\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # 预测\n",
    "    prediction = model.predict(padded)[0][0]\n",
    "    sentiment = \"正面评价\" if prediction > 0.5 else \"负面评价\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "def main():\n",
    "    print(\"加载和预处理数据...\")\n",
    "    train_data, test_data, tokenizer, max_length = load_and_preprocess_data()\n",
    "    \n",
    "    print(\"\\n创建Transformer模型...\")\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = create_transformer_model(vocab_size, max_length)\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"\\n开始训练...\")\n",
    "    history = train_model(model, train_data, test_data)\n",
    "    \n",
    "    # 测试新评论\n",
    "    test_texts = [\n",
    "        \"这个产品非常好用，超出我的预期\",\n",
    "        \"质量很差，客服态度也不好\",\n",
    "        # \"价格合理，性能还可以\",\n",
    "        # \"完全是浪费钱，后悔购买\",\n",
    "        # \"物流快，包装完好，推荐购买\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n预测新评论:\")\n",
    "    for text in test_texts:\n",
    "        sentiment, confidence = predict_sentiment(model, text, tokenizer, max_length)\n",
    "        print(f\"\\n文本: '{text}'\")\n",
    "        print(f\"预测: {sentiment} (置信度: {confidence:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多头注意力的工作过程：\n",
      "1. 输入词向量形状: (3, 4)\n",
      "2. 注意力输出形状: (1, 3, 4)\n",
      "\n",
      "每个词都会通过注意力机制关注其他词：\n",
      "\n",
      "对于词 '我':\n",
      "- 它会通过注意力机制与其他词 ['我', '喜欢', '机器学习'] 产生联系\n",
      "- 注意力分数表示它与每个词的关联程度\n",
      "\n",
      "对于词 '喜欢':\n",
      "- 它会通过注意力机制与其他词 ['我', '喜欢', '机器学习'] 产生联系\n",
      "- 注意力分数表示它与每个词的关联程度\n",
      "\n",
      "对于词 '机器学习':\n",
      "- 它会通过注意力机制与其他词 ['我', '喜欢', '机器学习'] 产生联系\n",
      "- 注意力分数表示它与每个词的关联程度\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Transformer块的处理过程：\n",
      "1. 输入序列通过多头注意力层\n",
      "2. 添加残差连接并进行层归一化\n",
      "3. 通过前馈网络\n",
      "4. 再次添加残差连接并进行层归一化\n",
      "\n",
      "输入形状: (1, 3, 4)\n",
      "输出形状: (1, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个简单的例子来展示注意力机制\n",
    "def show_attention_example():\n",
    "    # 假设我们有一个句子：\"我 喜欢 机器学习\"\n",
    "    sentence = [\"我\", \"喜欢\", \"机器学习\"]\n",
    "    \n",
    "    # 1. 创建简单的嵌入\n",
    "    word_embeddings = np.random.randn(3, 4)  # 3个词，每个词4维\n",
    "    \n",
    "    # 2. 创建多头注意力层\n",
    "    attention = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=2,  # 2个注意力头\n",
    "        key_dim=2     # 每个头的维度\n",
    "    )\n",
    "    \n",
    "    # 3. 计算注意力\n",
    "    # 扩展维度以匹配批处理要求\n",
    "    inputs = tf.expand_dims(word_embeddings, 0)  # [1, 3, 4]\n",
    "    attention_output = attention(inputs, inputs)\n",
    "    \n",
    "    # 4. 提取注意力权重\n",
    "    attention_weights = attention.get_weights()\n",
    "    \n",
    "    print(\"多头注意力的工作过程：\")\n",
    "    print(f\"1. 输入词向量形状: {word_embeddings.shape}\")\n",
    "    print(f\"2. 注意力输出形状: {attention_output.shape}\")\n",
    "    print(\"\\n每个词都会通过注意力机制关注其他词：\")\n",
    "    for i, word in enumerate(sentence):\n",
    "        print(f\"\\n对于词 '{word}':\")\n",
    "        print(f\"- 它会通过注意力机制与其他词 {sentence} 产生联系\")\n",
    "        print(\"- 注意力分数表示它与每个词的关联程度\")\n",
    "\n",
    "# 创建一个简单的Transformer块来说明完整的处理过程\n",
    "def create_simple_transformer_block(sequence_length=3, embedding_dim=4):\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, embedding_dim))\n",
    "    \n",
    "    # 1. 多头注意力\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=2,\n",
    "        key_dim=2\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    # 2. 第一个残差连接和层归一化\n",
    "    x = tf.keras.layers.LayerNormalization()(attention_output + inputs)\n",
    "    \n",
    "    # 3. 前馈网络\n",
    "    ffn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(embedding_dim)\n",
    "    ])(x)\n",
    "    \n",
    "    # 4. 第二个残差连接和层归一化\n",
    "    outputs = tf.keras.layers.LayerNormalization()(ffn + x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 展示完整的处理过程\n",
    "def show_transformer_process():\n",
    "    # 创建示例数据\n",
    "    batch_size = 1\n",
    "    sequence_length = 3\n",
    "    embedding_dim = 4\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_simple_transformer_block(sequence_length, embedding_dim)\n",
    "    \n",
    "    # 创建示例输入\n",
    "    example_input = tf.random.normal((batch_size, sequence_length, embedding_dim))\n",
    "    \n",
    "    # 获取输出\n",
    "    output = model(example_input)\n",
    "    \n",
    "    print(\"\\nTransformer块的处理过程：\")\n",
    "    print(\"1. 输入序列通过多头注意力层\")\n",
    "    print(\"2. 添加残差连接并进行层归一化\")\n",
    "    print(\"3. 通过前馈网络\")\n",
    "    print(\"4. 再次添加残差连接并进行层归一化\")\n",
    "    print(f\"\\n输入形状: {example_input.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "\n",
    "# 运行示例\n",
    "show_attention_example()\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "show_transformer_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 6.8714\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1558 - loss: 5.5860 \n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0875 - loss: 5.0656\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0771 - loss: 4.6878\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1083 - loss: 4.2631 \n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1083 - loss: 3.8840 \n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0875 - loss: 3.5157 \n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2446 - loss: 3.0933\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9050 - loss: 2.6112\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1209\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6426\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2038 \n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.8699 \n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6223 \n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.4818\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3685 \n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.2821 \n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.2213 \n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.1875\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.1664\n",
      "\n",
      "生成示例：\n",
      "\n",
      "种子文本: '今天天气'\n",
      "温度: 0.5\n",
      "生成结果: 今天天气 风景优美 服务态度也不错 风景优美 服务态度也不错 每天都要锻炼 服务态度也不错 服务态度也不错 服务态度也不错 <UNK> <UNK>\n",
      "\n",
      "种子文本: '今天天气'\n",
      "温度: 1.0\n",
      "生成结果: 今天天气 风景优美 开阔视野 阳光明媚 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "种子文本: '我很喜欢'\n",
      "温度: 0.5\n",
      "生成结果: 我很喜欢 风景优美 阳光明媚 服务态度也不错 开阔视野 风景优美 阳光明媚 服务态度也不错 服务态度也不错 <UNK> <UNK>\n",
      "\n",
      "种子文本: '我很喜欢'\n",
      "温度: 1.0\n",
      "生成结果: 我很喜欢 认真对待 阳光明媚 <UNK> 积极向上 开阔视野 可以创造很多东西 舒缓压力 认真对待 <UNK> <UNK>\n",
      "\n",
      "种子文本: '这家餐厅'\n",
      "温度: 0.5\n",
      "生成结果: 这家餐厅 风景优美 服务态度也不错 服务态度也不错 服务态度也不错 服务态度也不错 服务态度也不错 服务态度也不错 开阔视野 <UNK> <UNK>\n",
      "\n",
      "种子文本: '这家餐厅'\n",
      "温度: 1.0\n",
      "生成结果: 这家餐厅 每天都要锻炼 体验不同的文化 风景优美 服务态度也不错 认真对待 <UNK> <UNK> <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, vocab_size=5000, max_length=50, embedding_dim=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = None\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # 输入层\n",
    "        inputs = layers.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # 嵌入层\n",
    "        x = layers.Embedding(self.vocab_size, self.embedding_dim)(inputs)\n",
    "        \n",
    "        # 第一个Transformer块\n",
    "        x1 = layers.MultiHeadAttention(num_heads=8, key_dim=32)(x, x)\n",
    "        x1 = layers.LayerNormalization()(x + x1)\n",
    "        x1 = layers.Dropout(0.1)(x1)\n",
    "        \n",
    "        # 前馈网络\n",
    "        ffn = layers.Dense(512, activation='relu')(x1)\n",
    "        ffn = layers.Dense(self.embedding_dim)(ffn)\n",
    "        x1 = layers.LayerNormalization()(x1 + ffn)\n",
    "        \n",
    "        # 第二个Transformer块\n",
    "        x2 = layers.MultiHeadAttention(num_heads=8, key_dim=32)(x1, x1)\n",
    "        x2 = layers.LayerNormalization()(x1 + x2)\n",
    "        x2 = layers.Dropout(0.1)(x2)\n",
    "        \n",
    "        # 全局平均池化，将序列压缩为单个向量\n",
    "        x3 = layers.GlobalAveragePooling1D()(x2)\n",
    "        \n",
    "        # 输出层\n",
    "        outputs = layers.Dense(self.vocab_size, activation='softmax')(x3)\n",
    "        \n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def prepare_data(self, texts):\n",
    "        # 创建并训练tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        \n",
    "        # 创建训练序列\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            # 将文本转换为序列\n",
    "            sequence = self.tokenizer.texts_to_sequences([text])[0]\n",
    "            \n",
    "            # 创建输入-输出对\n",
    "            for i in range(1, len(sequence)):\n",
    "                input_seq = sequence[:i]\n",
    "                target = sequence[i]\n",
    "                sequences.append((input_seq, target))\n",
    "        \n",
    "        # 填充序列\n",
    "        X = []\n",
    "        y = []\n",
    "        for input_seq, target in sequences:\n",
    "            padded_seq = pad_sequences([input_seq], maxlen=self.max_length, padding='pre')[0]\n",
    "            X.append(padded_seq)\n",
    "            y.append(target)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train(self, texts, epochs=50):\n",
    "        # 准备数据\n",
    "        X, y = self.prepare_data(texts)\n",
    "        \n",
    "        # 训练模型\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=32)\n",
    "    \n",
    "    def generate_text(self, seed_text, max_gen_length=50, temperature=1.0):\n",
    "        # 将种子文本转换为序列\n",
    "        input_seq = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        generated_text = seed_text\n",
    "        \n",
    "        # 生成文本\n",
    "        for _ in range(max_gen_length):\n",
    "            # 填充序列\n",
    "            padded_seq = pad_sequences([input_seq], maxlen=self.max_length, padding='pre')\n",
    "            \n",
    "            try:\n",
    "                # 预测下一个词\n",
    "                predictions = self.model.predict(padded_seq, verbose=0)[0]  # 获取批次中的第一个预测\n",
    "                \n",
    "                # 应用温度采样\n",
    "                predictions = np.log(predictions) / temperature\n",
    "                exp_predictions = np.exp(predictions)\n",
    "                predictions = exp_predictions / np.sum(exp_predictions)\n",
    "                \n",
    "                # 确保predictions是一维的\n",
    "                predictions = predictions.flatten()\n",
    "                \n",
    "                # 采样下一个词\n",
    "                next_index = np.random.choice(len(predictions), p=predictions)\n",
    "                \n",
    "                # 将预测的词转换回文本\n",
    "                for word, index in self.tokenizer.word_index.items():\n",
    "                    if index == next_index:\n",
    "                        next_word = word\n",
    "                        break\n",
    "                else:\n",
    "                    next_word = '<UNK>'\n",
    "                \n",
    "                # 添加预测的词到生成的文本中\n",
    "                generated_text += \" \" + next_word\n",
    "                \n",
    "                # 更新输入序列\n",
    "                input_seq = list(input_seq)\n",
    "                input_seq.append(next_index)\n",
    "                if len(input_seq) > self.max_length:\n",
    "                    input_seq = input_seq[-self.max_length:]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"生成过程中出现错误: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"今天天气真好 阳光明媚\",\n",
    "    \"我很喜欢这个公园 风景优美\",\n",
    "    \"这家餐厅的菜很好吃 服务态度也不错\",\n",
    "    \"学习编程很有趣 可以创造很多东西\",\n",
    "    \"音乐能让人心情愉快 舒缓压力\",\n",
    "    \"运动对身体健康很重要 每天都要锻炼\",\n",
    "    \"读书可以增长知识 开阔视野\",\n",
    "    \"工作需要专注 认真对待\",\n",
    "    \"旅行可以见识不同的风景 体验不同的文化\",\n",
    "    \"生活中要保持乐观 积极向上\"\n",
    "] * 5  # 复制数据增加训练样本\n",
    "\n",
    "# 创建生成器\n",
    "generator = TextGenerator(vocab_size=1000, max_length=10, embedding_dim=128)\n",
    "\n",
    "# 训练模型\n",
    "print(\"开始训练模型...\")\n",
    "generator.train(texts, epochs=20)  # 减少epoch数避免过拟合\n",
    "\n",
    "# 生成文本\n",
    "print(\"\\n生成示例：\")\n",
    "seed_texts = [\n",
    "    \"今天天气\",\n",
    "    \"我很喜欢\",\n",
    "    \"这家餐厅\"\n",
    "]\n",
    "\n",
    "for seed in seed_texts:\n",
    "    # 使用不同的温度参数生成文本\n",
    "    for temp in [0.5, 1.0]:\n",
    "        try:\n",
    "            generated = generator.generate_text(\n",
    "                seed_text=seed,\n",
    "                max_gen_length=10,\n",
    "                temperature=temp\n",
    "            )\n",
    "            print(f\"\\n种子文本: '{seed}'\")\n",
    "            print(f\"温度: {temp}\")\n",
    "            print(f\"生成结果: {generated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"生成文本时出现错误: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "种子文本: '这家餐厅'\n",
      "温度: 0.5\n",
      "生成结果: 今天天气 风景优美 服务态度也不错 服务态度也不错 服务态度也不错 <UNK> 服务态度也不错 认真对待 服务态度也不错 <UNK> <UNK>\n",
      "\n",
      "种子文本: '这家餐厅'\n",
      "温度: 1.0\n",
      "生成结果: 今天天气 舒缓压力 风景优美 服务态度也不错 体验不同的文化 <UNK> 服务态度也不错 服务态度也不错 <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for temp in [0.5, 1.0]:\n",
    "    try:\n",
    "        generated = generator.generate_text(\n",
    "            seed_text=\"今天天气\",\n",
    "            max_gen_length=10,\n",
    "            temperature=temp\n",
    "        )\n",
    "        print(f\"\\n种子文本: '{seed}'\")\n",
    "        print(f\"温度: {temp}\")\n",
    "        print(f\"生成结果: {generated}\")\n",
    "    except Exception as e:\n",
    "        print(f\"生成文本时出现错误: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
