{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据...\n",
      "下载数据集...\n",
      "下载失败: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/weibo_senti_100k/weibo_senti_100k.csv\n",
      "词汇表大小: 10\n",
      "训练数据形状: (8, 100)\n",
      "测试数据形状: (2, 100)\n",
      "\n",
      "模型结构:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyiwen/Documents/ai/2022-Machine-Learning-Specializatio/venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练...\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5714 - loss: 0.6936 - val_accuracy: 1.0000 - val_loss: 0.6732 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7143 - loss: 0.6877 - val_accuracy: 1.0000 - val_loss: 0.6402 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7143 - loss: 0.6873 - val_accuracy: 1.0000 - val_loss: 0.6136 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7143 - loss: 0.6561 - val_accuracy: 1.0000 - val_loss: 0.5809 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7143 - loss: 0.6472 - val_accuracy: 1.0000 - val_loss: 0.5347 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7143 - loss: 0.6718 - val_accuracy: 1.0000 - val_loss: 0.4842 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7143 - loss: 0.6002 - val_accuracy: 1.0000 - val_loss: 0.4146 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7143 - loss: 0.6042 - val_accuracy: 1.0000 - val_loss: 0.3377 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7143 - loss: 0.5594 - val_accuracy: 1.0000 - val_loss: 0.2628 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7143 - loss: 0.5874 - val_accuracy: 1.0000 - val_loss: 0.2215 - learning_rate: 0.0010\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 1.6161\n",
      "\n",
      "测试集准确率: 0.0000\n",
      "\n",
      "预测新评论:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\n",
      "文本: '这个产品非常好用，超出我的预期'\n",
      "预测: 正面评价 (置信度: 0.8013)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "文本: '质量很差，客服态度也不好'\n",
      "预测: 正面评价 (置信度: 0.8013)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "文本: '价格合理，性能还可以'\n",
      "预测: 正面评价 (置信度: 0.8013)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "文本: '完全是浪费钱，后悔购买'\n",
      "预测: 正面评价 (置信度: 0.8013)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文本: '物流快，包装完好，推荐购买'\n",
      "预测: 正面评价 (置信度: 0.8013)\n",
      "\n",
      "模型已保存为 'sentiment_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "class WeiboSentimentDataset:\n",
    "    def __init__(self):\n",
    "        # 数据集URL\n",
    "        self.url = \"https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/weibo_senti_100k/weibo_senti_100k.csv\"\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        try:\n",
    "            print(\"下载数据集...\")\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()  # 检查下载是否成功\n",
    "            return pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "        except Exception as e:\n",
    "            print(f\"下载失败: {e}\")\n",
    "            # 如果下载失败，使用小型示例数据集\n",
    "            return self.get_sample_dataset()\n",
    "    \n",
    "    def get_sample_dataset(self):\n",
    "        # 示例数据，实际项目中应该使用更大的数据集\n",
    "        data = {\n",
    "            'review': [\n",
    "                \"这个产品很好用，我很喜欢\",\n",
    "                \"质量特别差，退货了\",\n",
    "                \"一般般，可以接受\",\n",
    "                \"很满意，物超所值\",\n",
    "                \"不推荐购买，浪费钱\",\n",
    "                # ... 添加更多示例\n",
    "                \"服务态度很好，下次还会购买\",\n",
    "                \"出现故障，客服态度很差\",\n",
    "                \"性价比很高，推荐购买\",\n",
    "                \"完全不值这个价格\",\n",
    "                \"快递很快，产品完好\",\n",
    "            ],\n",
    "            'label': [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # 加载数据集\n",
    "    dataset = WeiboSentimentDataset()\n",
    "    df = dataset.download_dataset()\n",
    "    \n",
    "    # 分割训练集和测试集\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df['review'].values, \n",
    "        df['label'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 创建tokenizer\n",
    "    tokenizer = Tokenizer(num_words=50000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # 转换文本为序列\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "    \n",
    "    # 设置最大序列长度\n",
    "    max_length = 100\n",
    "    \n",
    "    # 填充序列\n",
    "    X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "    X_test = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    print(f\"词汇表大小: {len(tokenizer.word_index) + 1}\")\n",
    "    print(f\"训练数据形状: {X_train.shape}\")\n",
    "    print(f\"测试数据形状: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, train_labels, X_test, test_labels, tokenizer, max_length\n",
    "\n",
    "def build_improved_lstm_model(vocab_size, max_length):\n",
    "    model = Sequential([\n",
    "        # 嵌入层\n",
    "        Embedding(vocab_size, 128, input_length=max_length),\n",
    "        \n",
    "        # 第一个LSTM层\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # 第二个LSTM层\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # 全连接层\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=20):\n",
    "    # 创建验证集\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 定义回调函数\n",
    "    callbacks = [\n",
    "        # 早停\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # 学习率调整\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(\n",
    "        X_train_split, y_train_split,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 评估模型\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\n测试集准确率: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return history, test_accuracy\n",
    "\n",
    "def predict_sentiment(model, text, tokenizer, max_length):\n",
    "    # 预处理文本\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # 预测\n",
    "    prediction = model.predict(padded)[0][0]\n",
    "    sentiment = \"正面评价\" if prediction > 0.5 else \"负面评价\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "def main():\n",
    "    # 加载和预处理数据\n",
    "    print(\"开始加载数据...\")\n",
    "    X_train, y_train, X_test, y_test, tokenizer, max_length = load_and_preprocess_data()\n",
    "    \n",
    "    # 构建模型\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = build_improved_lstm_model(vocab_size, max_length)\n",
    "    print(\"\\n模型结构:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"\\n开始训练...\")\n",
    "    history, test_accuracy = train_model(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        batch_size=32,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    # 测试新评论\n",
    "    test_texts = [\n",
    "        \"这个产品非常好用，超出我的预期\",\n",
    "        \"质量很差，客服态度也不好\",\n",
    "        \"价格合理，性能还可以\",\n",
    "        \"完全是浪费钱，后悔购买\",\n",
    "        \"物流快，包装完好，推荐购买\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n预测新评论:\")\n",
    "    for text in test_texts:\n",
    "        sentiment, confidence = predict_sentiment(model, text, tokenizer, max_length)\n",
    "        print(f\"\\n文本: '{text}'\")\n",
    "        print(f\"预测: {sentiment} (置信度: {confidence:.4f})\")\n",
    "        \n",
    "    # 保存模型\n",
    "    model.save('sentiment_model.h5')\n",
    "    print(\"\\n模型已保存为 'sentiment_model.h5'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
